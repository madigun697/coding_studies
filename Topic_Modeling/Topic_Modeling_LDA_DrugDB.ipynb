{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TsKdaZaYb6S"
   },
   "source": [
    "# Topic Modeling using LDA\n",
    "\n",
    "### References\n",
    "\n",
    "* Data: Drug Dataset (400EA)\n",
    "* Preprocess: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "* LDA: https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/09/lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBHXrVXj9rsK"
   },
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import article_db_conn\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>table_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Table 2 Clinical, Doppler Echocardiographic and Hemodynamic Characteristics in the Patients Grouped According to Baseline MFP and Acute Change After Loading Manipulations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Table 3 Association (Multivariate Logistic Regression Analysis) Between Either Baseline MFP or Its Acute Change After Loading Manipulations and Tolerance to Beta-Blocker Treatment, Adjusting for Gender, Etiology, NYHA Class, LVEF and Peak VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Table 1 Baseline and Clinical Characteristics of the Two Patient Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Table 2 Clinical Course of the Two Patient Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Table 3 Characteristics and In-Hospital Outcome of the Study and the Registry Patients</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      2   \n",
       "3      3   \n",
       "4      4   \n",
       "\n",
       "                                                                                                                                                                                                                                          table_title  \n",
       "0                                                                          Table 2 Clinical, Doppler Echocardiographic and Hemodynamic Characteristics in the Patients Grouped According to Baseline MFP and Acute Change After Loading Manipulations  \n",
       "1  Table 3 Association (Multivariate Logistic Regression Analysis) Between Either Baseline MFP or Its Acute Change After Loading Manipulations and Tolerance to Beta-Blocker Treatment, Adjusting for Gender, Etiology, NYHA Class, LVEF and Peak VO2  \n",
       "2                                                                                                                                                                             Table 1 Baseline and Clinical Characteristics of the Two Patient Groups  \n",
       "3                                                                                                                                                                                                   Table 2 Clinical Course of the Two Patient Groups  \n",
       "4                                                                                                                                                              Table 3 Characteristics and In-Hospital Outcome of the Study and the Registry Patients  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_titles = article_db_conn.select_query(\"SELECT table_title FROM article_tables\")\n",
    "columns = ['table_title']\n",
    "article_titles_pd = pd.DataFrame(article_titles, columns=columns).reset_index()\n",
    "article_titles_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'Table [0-9]* ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles_pd.table_title = article_titles_pd.table_title.apply(lambda x: x.replace(regex.search(x).group(), '') if regex.search(x) != None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>table_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Clinical, Doppler Echocardiographic and Hemodynamic Characteristics in the Patients Grouped According to Baseline MFP and Acute Change After Loading Manipulations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Association (Multivariate Logistic Regression Analysis) Between Either Baseline MFP or Its Acute Change After Loading Manipulations and Tolerance to Beta-Blocker Treatment, Adjusting for Gender, Etiology, NYHA Class, LVEF and Peak VO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Baseline and Clinical Characteristics of the Two Patient Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Clinical Course of the Two Patient Groups</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Characteristics and In-Hospital Outcome of the Study and the Registry Patients</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      2   \n",
       "3      3   \n",
       "4      4   \n",
       "\n",
       "                                                                                                                                                                                                                                  table_title  \n",
       "0                                                                          Clinical, Doppler Echocardiographic and Hemodynamic Characteristics in the Patients Grouped According to Baseline MFP and Acute Change After Loading Manipulations  \n",
       "1  Association (Multivariate Logistic Regression Analysis) Between Either Baseline MFP or Its Acute Change After Loading Manipulations and Tolerance to Beta-Blocker Treatment, Adjusting for Gender, Etiology, NYHA Class, LVEF and Peak VO2  \n",
       "2                                                                                                                                                                             Baseline and Clinical Characteristics of the Two Patient Groups  \n",
       "3                                                                                                                                                                                                   Clinical Course of the Two Patient Groups  \n",
       "4                                                                                                                                                              Characteristics and In-Hospital Outcome of the Study and the Registry Patients  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_titles_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_GA1U3fAECz"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1541566041043,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EuX1F67TYXFc",
    "outputId": "c2c74f56-a6fb-466b-bb63-28a117cb62d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gracelee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LO8jzrS7AIrp"
   },
   "source": [
    "* Preprocess\n",
    " 1. simple_preprocess: Split Text by whitespace\n",
    " 2. STOPWORDS: Remove stopwords\n",
    " 3. lemmatize_stemming\n",
    " \n",
    "* lemmatize_stemming\n",
    " - Lemmatizing & Stemming Replace word with original form\n",
    " - Lemmatizing consider whether the word exist in the real world\n",
    " - pos means a position of the word\n",
    " - https://m.blog.naver.com/PostView.nhn?blogId=vangarang&logNo=220963244354&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V0FdvKJZezi"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go6me5u-TzlG"
   },
   "source": [
    "* Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 164587,
     "status": "ok",
     "timestamp": 1541566208089,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EGbICJX5aIMg",
    "outputId": "f32a035a-c555-4056-86ea-c971f2e07337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 31.1 ms, total: 14.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                                                          [clinic, doppler, hemodynam, characterist, patient, group, accord, baselin, acut, chang, load, manipul]\n",
       "1                                                            [associ, multivari, logist, regress, analysi, baselin, acut, chang, load, manipul, toler, beta, blocker, treatment, adjust, gender, etiolog, nyha, class, lvef, peak]\n",
       "2                                                                                                                                                                                  [baselin, clinic, characterist, patient, group]\n",
       "3                                                                                                                                                                                                  [clinic, cours, patient, group]\n",
       "4                                                                                                                                                                         [characterist, hospit, outcom, studi, registri, patient]\n",
       "5                                                                                                                                                     [comparison, demograph, baselin, clinic, data, devic, surgic, closur, group]\n",
       "6                                                                                                                                                                               [comparison, outcom, devic, surgic, closur, group]\n",
       "7    [associ, multivari, logist, regress, analysi, baselin, acut, chang, load, manipul, improv, lvef, deceler, time, mitral, regurgit, month, beta, blocker, treatment, adjust, gender, etiolog, nyha, class, baselin, lvef, peak]\n",
       "8                                                                                                                                                                                           [major, advers, cardiovascular, event]\n",
       "9                                                                                                                                                                              [comparison, complic, devic, surgic, closur, group]\n",
       "Name: table_title, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time processed_docs = article_titles_pd['table_title'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "* https://lumiamitie.github.io/r/python/tsne-for-r-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TSNE모델에는 transform 메소드가 없고 fit_transform만 있음\n",
    "# library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 582 ms, sys: 12.7 ms, total: 594 ms\n",
      "Wall time: 594 ms\n",
      "CPU times: user 651 ms, sys: 319 ms, total: 970 ms\n",
      "Wall time: 974 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "%time vect.fit([' '.join(d) for d in processed_docs])\n",
    "%time tsne_data = vect.transform([' '.join(d) for d in processed_docs]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tsne_result = TSNE(learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 시각화\n",
    "# plt.scatter(tsne_result[:, 1], tsne_result[:, 0])\n",
    "# plt.xlim(tsne_result[:, 1].min()-3, tsne_result[:, 1].max()+3) # 최소, 최대\n",
    "# plt.ylim(tsne_result[:, 0].min()-3, tsne_result[:, 0].max()+3) # 최소, 최대\n",
    "# plt.xlabel('t-SNE 특성0') # x축 이름\n",
    "# plt.ylabel('t-SNE 특성1') # y축 이름\n",
    "# plt.show() # 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./drug_db_tsne_result.pkl', 'wb') as f:\n",
    "    pickle.dump(tsne_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tsne_3d_result = TSNE(n_components=3, learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_3d_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "# plt.rcParams['lines.linewidth'] = 1\n",
    "# plt.rcParams['lines.color'] = 'r'\n",
    "# plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for x, y, z in tsne_3d_result:\n",
    "#     ax.scatter(x, y, z, c='blue')\n",
    "    \n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgzBTvekcz43"
   },
   "source": [
    "### LDA\n",
    "\n",
    "* Setting Variables\n",
    "\n",
    "    1. document_topic_counts : List of Counter (len = count of documents)\n",
    "    2. topic_word_counts : List of Counter (len = count of topic)\n",
    "    3. topic_counts : List of Integer (len = count of topic)\n",
    "    4. document_lengths : List of length of documents\n",
    "    5. distinct_words: All unique words in dataset\n",
    "    6. V: length of distinct words\n",
    "    7. D: length of documents\n",
    "    \n",
    "* Counter Object\n",
    " - Calculate count of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9LRe-K8bcKK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_variables(K):\n",
    "    # 사용자가 원하는 토픽의 갯수\n",
    "    K = 8\n",
    "\n",
    "    # 각 토픽이 각 문서에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 문서를 의미\n",
    "    document_topic_counts = [Counter() for _ in processed_docs]\n",
    "\n",
    "    # 각 단어가 각 토픽에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 토픽을 의미\n",
    "    topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "    # 각 토픽에 할당되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 토픽을 의미함\n",
    "    topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "    # 각 문서에 포함되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 문서를 의미함\n",
    "    document_lengths = list(map(len, processed_docs))\n",
    "\n",
    "    # 단어 종류의 수\n",
    "    distinct_words = set(word for document in processed_docs for word in document)\n",
    "    V = len(distinct_words)\n",
    "\n",
    "    # 총 문서의 수\n",
    "    D = len(processed_docs)\n",
    "\n",
    "    return V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7qgzPvO7kTP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gjsUyxxc4fh"
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3jga4rC7krT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI23O0lUdVO2"
   },
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5lgknpvh5ba"
   },
   "source": [
    "* Run\n",
    " - Initialize Topic using random value by word in documents\n",
    " - Calculate variables\n",
    "    1. document_topic_counts\n",
    "        - count of topic word in every document\n",
    "        - 개별 문서에서 topic word의 등장 횟수\n",
    "    2. topic_word_counts\n",
    "        - appearance count of words in whole documents\n",
    "        - every word seperate by topic\n",
    "        - 개별 Topic에서 topic word의 등장 횟수(전체 문서 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_ViLSFS5bwX"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "K = 8\n",
    "V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words = get_variables(K)\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document] for document in processed_docs]\n",
    "\n",
    "# 위와 같이 랜덤 초기화한 상태에서 \n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(processed_docs[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1541572007570,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "z_qa8yOb2fyC",
    "outputId": "f224e65b-0390-4487-fdcb-22488ff00697"
   },
   "outputs": [],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1400979,
     "status": "ok",
     "timestamp": 1541577464691,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "jxqPxYZv7Usi",
    "outputId": "a82e7172-17a9-4db3-9ca3-568fa8de2b99"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time() \n",
    "\n",
    "for iter in range(3000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(processed_docs[d], document_topics[d])):\n",
    "            # 깁스 샘플링 수행을 위해\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한 \n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # 샘플링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "    \n",
    "    if iter % 500 == 0:\n",
    "        print(\"--- %d iter: %s mins ---\" % (iter, str((time.time() - start_time) / 60.)))\n",
    "\n",
    "print(\"--- %s mins ---\" % str((time.time() - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.color'] = 'r'\n",
    "plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# doc_result.plot.scatter(x='plot_x', y='plot_y', c='topic', colormap='Accent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threedee = plt.figure().gca(projection='3d')\n",
    "# threedee.scatter(doc_result.td_x, doc_result.td_y, doc_result.td_z, c=doc_result.topic)\n",
    "\n",
    "# plt.savefig('3d_scatter_lda.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Glk0FqEpvIBU"
   },
   "source": [
    "* 깁스 샘플링(Gibbs Sampling) \n",
    "    * http://4four.us/article/2014/10/lda-parameter-estimation\n",
    "    * https://bab2min.tistory.com/569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyLDAvis\n",
    "    * https://lovit.github.io/nlp/2018/09/27/pyldavis_lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ndarray, shape = (n_topics, n_terms)\n",
    "topic_term_dists = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))) \n",
    "\n",
    "# numpy.ndarray, shape = (n_docs, n_topics)\n",
    "doc_topic_dists = pd.DataFrame([d.values() for d in document_topic_counts]).fillna(0).values\n",
    "\n",
    "# numpy.ndarray, shape = (n_docs,)\n",
    "doc_lengths = np.array(document_lengths)\n",
    "\n",
    "# list of str, vocab list\n",
    "vocab = list(distinct_words)\n",
    "\n",
    "# numpy.ndarray, shape = (n_vocabs,)\n",
    "term_frequency = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* topic_term_dists: topic_term_dists\n",
    "* doc_topic_dists: doc_topic_dists\n",
    "* doc_lengths: doc_lengths\n",
    "* vocab: vocab\n",
    "* term_frequency: term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_mallet_data = {\n",
    "    'topic_term_dists':topic_term_dists,\n",
    "    'doc_topic_dists':doc_topic_dists,\n",
    "    'doc_lengths':doc_lengths,\n",
    "    'vocab':vocab,\n",
    "    'term_frequency':term_frequency\n",
    "}\n",
    "vis_data = pyLDAvis.prepare(**lda_mallet_data)\n",
    "# pyLDAvis.display(vis_data)\n",
    "# pyLDAvis.save_html(vis_data, 'test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Default' 'Topic1' 'Topic2' 'Topic3' 'Topic4' 'Topic5' 'Topic6' 'Topic7'\n",
      " 'Topic8']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Term</th>\n",
       "      <th>Total</th>\n",
       "      <th>loglift</th>\n",
       "      <th>logprob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>503910.0</td>\n",
       "      <td>regress</td>\n",
       "      <td>503910.0</td>\n",
       "      <td>8.4280</td>\n",
       "      <td>3.4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>473370.0</td>\n",
       "      <td>model</td>\n",
       "      <td>548574.0</td>\n",
       "      <td>8.2806</td>\n",
       "      <td>3.4340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>366480.0</td>\n",
       "      <td>hazard</td>\n",
       "      <td>426575.0</td>\n",
       "      <td>8.2762</td>\n",
       "      <td>3.1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>351210.0</td>\n",
       "      <td>multivari</td>\n",
       "      <td>361067.0</td>\n",
       "      <td>8.4004</td>\n",
       "      <td>3.1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>351210.0</td>\n",
       "      <td>analysi</td>\n",
       "      <td>638765.0</td>\n",
       "      <td>7.8299</td>\n",
       "      <td>3.1355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category      Freq       Term     Total  loglift  logprob\n",
       "term                                                          \n",
       "291    Topic1  503910.0    regress  503910.0   8.4280   3.4965\n",
       "656    Topic1  473370.0      model  548574.0   8.2806   3.4340\n",
       "749    Topic1  366480.0     hazard  426575.0   8.2762   3.1781\n",
       "1185   Topic1  351210.0  multivari  361067.0   8.4004   3.1355\n",
       "768    Topic1  351210.0    analysi  638765.0   7.8299   3.1355"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDAvis의 우측 HBar Chart Data\n",
    "# Freq: Estimated term frequency within the selected topic\n",
    "# Total: Overall term frequency\n",
    "print(vis_data.topic_info.Category.unique())\n",
    "vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### 1. Main View\n",
    "* Layout: https://www.codingfactory.net/10530\n",
    "\n",
    "#### a. HBar Chart\n",
    "* Data: vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()\n",
    "* D3: http://bl.ocks.org/erikvullings/51cc5332439939f1f292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hbar_json = {}\n",
    "hbar_json['labels'] = vis_data.topic_info.Category.unique().tolist()\n",
    "hbar_json['max_width'] = vis_data.topic_info[vis_data.topic_info.Category != 'Default'][['Total']].max()[0]\n",
    "for l in vis_data.topic_info.Category.unique().tolist():\n",
    "    tmp_df = vis_data.topic_info[vis_data.topic_info.Category == l].sort_values(['Category', 'Freq'], ascending=[True, False]).groupby('Category').head()\n",
    "    sub_json = {}\n",
    "\n",
    "    hbar_json[l] = list(tmp_df[['Term', 'Freq', 'Total']].sort_values('Freq', ascending=False).reset_index().to_dict('index').values())\n",
    "    \n",
    "f = open('./Visualization/res/lda/hbar_data.json', 'w')\n",
    "f.write(json.dumps(hbar_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Scatter Chart\n",
    "* Data: tsne_result\n",
    "* D3: https://bl.ocks.org/Niekes/1c15016ae5b5f11508f92852057136b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_json = list(doc_result[['id', 'plot_x', 'plot_y', 'topic']].to_dict('index').values())\n",
    "\n",
    "f = open('./Visualization/res/lda/scatter_data.json', 'w')\n",
    "f.write(json.dumps(scatter_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Table\n",
    "* Data: doc_result[['topic', 'document']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result.to_csv('./data_output/lda.tsv', sep='\\t', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>coronari(47),arteri(17),patient(16),wall(15),detect(15),angiographi(13),flow(13),diagnost(13),segment(12),comput(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>event(78),advers(70),treatment(66),grade(30),group(29),popul(28),patient(26),safeti(22),emerg(22),relat(17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>7</td>\n",
       "      <td>patient(57),outcom(24),hospit(22),acut(21),clinic(21),legend(20),myocardi(18),infarct(17),therapi(13),versus(11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>4</td>\n",
       "      <td>year(36),death(34),caus(33),rat(19),efficaci(19),specif(17),mortal(16),relat(16),adjust(16),chang(13)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic  \\\n",
       "0        0   \n",
       "31       5   \n",
       "50       1   \n",
       "51       2   \n",
       "66       6   \n",
       "69       3   \n",
       "150      7   \n",
       "195      4   \n",
       "\n",
       "                                                                                                               topic_word  \n",
       "0    regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)  \n",
       "31   coronari(47),arteri(17),patient(16),wall(15),detect(15),angiographi(13),flow(13),diagnost(13),segment(12),comput(12)  \n",
       "50                 intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)  \n",
       "51          cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)  \n",
       "66            event(78),advers(70),treatment(66),grade(30),group(29),popul(28),patient(26),safeti(22),emerg(22),relat(17)  \n",
       "69              risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)  \n",
       "150      patient(57),outcom(24),hospit(22),acut(21),clinic(21),legend(20),myocardi(18),infarct(17),therapi(13),versus(11)  \n",
       "195                 year(36),death(34),caus(33),rat(19),efficaci(19),specif(17),mortal(16),relat(16),adjust(16),chang(13)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').head(1)[['topic', 'topic_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 183, 184, 220, 240, 262, 263, 264, 266, 268, 270, 272, 274, 276, 278, 286, 288, 290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[50, 52, 53, 54, 55, 60, 62, 64, 65, 68, 70, 71, 72, 76, 77, 79, 80, 81, 83, 85, 86, 87, 88, 89, 91, 93, 94, 96, 98, 99, 255, 366, 394]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[51, 56, 57, 58, 59, 61, 63, 67, 73, 74, 84, 200, 203, 204, 205, 206, 208, 212, 213, 215, 218, 221, 223, 225, 226, 231, 233, 236, 244, 248, 284, 292, 386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[69, 163, 251, 256, 280, 282, 283, 289, 296, 297, 298, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 387, 388, 389, 390, 391, 392, 393, 395, 396, 397, 398, 399]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[195, 201, 202, 207, 209, 210, 211, 214, 216, 217, 219, 222, 224, 227, 228, 229, 230, 232, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 249, 257, 260, 265, 269, 279, 287, 291, 293, 294]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[31, 37, 250, 253, 258, 261, 277, 285, 300, 303, 304, 305, 306, 307, 308, 309, 310, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 385]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[66, 75, 78, 82, 90, 92, 95, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 254, 271, 273, 299, 301, 302, 318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 252, 259, 267, 275, 281, 295, 311, 312, 321, 375]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                  id\n",
       "topic                                                                                                                                                                                                                                                                                                                               \n",
       "0                                                        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 183, 184, 220, 240, 262, 263, 264, 266, 268, 270, 272, 274, 276, 278, 286, 288, 290]\n",
       "1                                                                                                                                                                                            [50, 52, 53, 54, 55, 60, 62, 64, 65, 68, 70, 71, 72, 76, 77, 79, 80, 81, 83, 85, 86, 87, 88, 89, 91, 93, 94, 96, 98, 99, 255, 366, 394]\n",
       "2                                                                                                                                                                         [51, 56, 57, 58, 59, 61, 63, 67, 73, 74, 84, 200, 203, 204, 205, 206, 208, 212, 213, 215, 218, 221, 223, 225, 226, 231, 233, 236, 244, 248, 284, 292, 386]\n",
       "3                                            [69, 163, 251, 256, 280, 282, 283, 289, 296, 297, 298, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 387, 388, 389, 390, 391, 392, 393, 395, 396, 397, 398, 399]\n",
       "4                                                                                                                                [195, 201, 202, 207, 209, 210, 211, 214, 216, 217, 219, 222, 224, 227, 228, 229, 230, 232, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 249, 257, 260, 265, 269, 279, 287, 291, 293, 294]\n",
       "5                                                            [31, 37, 250, 253, 258, 261, 277, 285, 300, 303, 304, 305, 306, 307, 308, 309, 310, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 385]\n",
       "6      [66, 75, 78, 82, 90, 92, 95, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 254, 271, 273, 299, 301, 302, 318]\n",
       "7                                           [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 252, 259, 267, 275, 281, 295, 311, 312, 321, 375]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').agg({'id': 'unique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Topic_Modelling_LDA_ABC_News.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
