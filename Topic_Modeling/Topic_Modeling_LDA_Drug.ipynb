{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TsKdaZaYb6S"
   },
   "source": [
    "# Topic Modeling using LDA\n",
    "\n",
    "### References\n",
    "\n",
    "* Data: Drug Dataset (400EA)\n",
    "* Preprocess: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "* LDA: https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/09/lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBHXrVXj9rsK"
   },
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ut-GgvMmTiG6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "news_data = pd.read_csv('./mallet_top_sen.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1541566038639,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "B6iJDggyXxMH",
    "outputId": "5391d2f4-6a96-4566-c58e-925f7375d47a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contribu</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>44029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>['hazard', 'ratio', 'confid', 'interv', 'univari', 'multivari', 'analysi', 'predictor', 'major', 'cardiac', 'event', 'cardiac', 'death', 'worsen', 'heart', 'failur', 'lead', 'heart', 'transplant']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2836</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>['leav', 'ventricular', 'right', 'ventricular', 'eject', 'fraction', 'leav', 'ventricular', 'right', 'ventricular', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'ventricl', 'case', 'leav', 'side', 'wpw']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2817</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>['partial', 'regress', 'coeffici', 'subject', 'forward', 'stepwis', 'linear', 'regress', 'depend', 'variabl', 'augment', 'pressur', 'augment', 'index']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2797</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>['leav', 'ventricular', 'lvef', 'right', 'ventricular', 'rvef', 'eject', 'fraction', 'leav', 'ventricular', 'lvmp', 'right', 'ventricular', 'rvmp', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'lvpsd', 'rvpsd', 'ventricl', 'case', 'right', 'side', 'wpw']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>['predictor', 'mortal', 'multivari', 'analysi', 'variabl', 'show', 'order', 'enter', 'stepwis', 'cox', 'regress', 'model']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  Topic_Num  Topic_Perc_Contribu  \\\n",
       "0           0  44029        0.0               0.2935   \n",
       "1           1  23344        0.0               0.2836   \n",
       "2           2  41163        0.0               0.2817   \n",
       "3           3  23343        0.0               0.2797   \n",
       "4           4  24968        0.0               0.2782   \n",
       "\n",
       "                                                                          Topic_Keywords  \\\n",
       "0  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "1  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "2  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "3  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "4  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                   Text  \n",
       "0                                                                                                                  ['hazard', 'ratio', 'confid', 'interv', 'univari', 'multivari', 'analysi', 'predictor', 'major', 'cardiac', 'event', 'cardiac', 'death', 'worsen', 'heart', 'failur', 'lead', 'heart', 'transplant']  \n",
       "1                                                     ['leav', 'ventricular', 'right', 'ventricular', 'eject', 'fraction', 'leav', 'ventricular', 'right', 'ventricular', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'ventricl', 'case', 'leav', 'side', 'wpw']  \n",
       "2                                                                                                                                                               ['partial', 'regress', 'coeffici', 'subject', 'forward', 'stepwis', 'linear', 'regress', 'depend', 'variabl', 'augment', 'pressur', 'augment', 'index']  \n",
       "3  ['leav', 'ventricular', 'lvef', 'right', 'ventricular', 'rvef', 'eject', 'fraction', 'leav', 'ventricular', 'lvmp', 'right', 'ventricular', 'rvmp', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'lvpsd', 'rvpsd', 'ventricl', 'case', 'right', 'side', 'wpw']  \n",
       "4                                                                                                                                                                                            ['predictor', 'mortal', 'multivari', 'analysi', 'variabl', 'show', 'order', 'enter', 'stepwis', 'cox', 'regress', 'model']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePl8Ln9b_8NV"
   },
   "source": [
    "#### Extract target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1541566040044,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "aFIUgkTfZ2Jl",
    "outputId": "a537fb6a-b4a6-4a96-be3e-c4676af07360"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = news_data[['Origin_Text']]\n",
    "data_text['index'] = news_data[['Unnamed: 0']]\n",
    "documents = data_text\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_GA1U3fAECz"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: bz2file in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.45 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1541566041043,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EuX1F67TYXFc",
    "outputId": "c2c74f56-a6fb-466b-bb63-28a117cb62d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gracelee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LO8jzrS7AIrp"
   },
   "source": [
    "* Preprocess\n",
    " 1. simple_preprocess: Split Text by whitespace\n",
    " 2. STOPWORDS: Remove stopwords\n",
    " 3. lemmatize_stemming\n",
    " \n",
    "* lemmatize_stemming\n",
    " - Lemmatizing & Stemming Replace word with original form\n",
    " - Lemmatizing consider whether the word exist in the real world\n",
    " - pos means a position of the word\n",
    " - https://m.blog.naver.com/PostView.nhn?blogId=vangarang&logNo=220963244354&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V0FdvKJZezi"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xJK8DKBJvSq"
   },
   "source": [
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1541566043352,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "AkEpG_ey86-3",
    "outputId": "f4f392d1-d5b0-436e-db0b-2fee33d68ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Treatment', 'efficacy', 'at', 'week', '36', 'for', 'the', 'modified', 'intention-to-treat', 'population', 'in', 'the', 'open-label', 'period', 'and', 'at', 'week', '88', 'for', 'the', 'modified', 'intention-to-treat', 'subpopulations', 'in', 'the', 'double-blind', 'period']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['treatment', 'efficaci', 'week', 'modifi', 'intent', 'treat', 'popul', 'open', 'label', 'period', 'week', 'modifi', 'intent', 'treat', 'subpopul', 'doubl', 'blind', 'period']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 100].values[0][0]\n",
    "print('original document: ')\n",
    "\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go6me5u-TzlG"
   },
   "source": [
    "* Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 164587,
     "status": "ok",
     "timestamp": 1541566208089,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EGbICJX5aIMg",
    "outputId": "f32a035a-c555-4056-86ea-c971f2e07337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 612 ms, sys: 3.56 ms, total: 615 ms\n",
      "Wall time: 615 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                        [hazard, ratio, confid, interv, univari, multivari, analysi, predictor, major, cardiac, event, cardiac, death, worsen, heart, failur, lead, heart, transplant]\n",
       "1                                           [leav, ventricular, right, ventricular, eject, fraction, leav, ventricular, right, ventricular, mean, phase, leav, right, mean, phase, differ, phase, standard, deviat, ventricl, case, leav, side, legend]\n",
       "2                                                                                                                   [partial, regress, coeffici, subject, forward, stepwis, linear, regress, depend, variabl, augment, pressur, augment, index, legend]\n",
       "3    [leav, ventricular, lvef, right, ventricular, rvef, eject, fraction, leav, ventricular, lvmp, right, ventricular, rvmp, mean, phase, leav, right, mean, phase, differ, phase, standard, deviat, lvpsd, rvpsd, ventricl, case, right, side, legend]\n",
       "4                                                                                                                                                         [predictor, mortal, multivari, analysi, variabl, show, order, enter, stepwis, regress, model]\n",
       "5                                                                [logist, regress, model, post, transplant, predictor, normal, leav, ventricular, eject, fraction, lvef, post, transplant, period, refer, group, group, lvef, post, transplant, period]\n",
       "6                                                                                                        [result, surviv, analysi, proport, hazard, model, combin, point, cardiac, mortal, urgent, heart, transplant, readmiss, congest, heart, failur]\n",
       "7                                                                                                    [impact, normal, versus, reduc, leav, ventricular, eject, fraction, mortal, congest, heart, failur, case, result, proport, hazard, regress, model]\n",
       "8                                                                                               [descript, leav, ventricular, eject, fraction, lvef, differ, time, period, time, transplant, evalu, wait, list, transplant, follow, kidney, transplant]\n",
       "9                                                                                                                                                              [univari, regress, analysi, proport, hazard, analysi, predictor, cardiac, death, legend]\n",
       "Name: Origin_Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time processed_docs = documents['Origin_Text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "* https://lumiamitie.github.io/r/python/tsne-for-r-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TSNE모델에는 transform 메소드가 없고 fit_transform만 있음\n",
    "# library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents['Origin_Text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 ms, sys: 1.84 ms, total: 29.1 ms\n",
      "Wall time: 28.1 ms\n",
      "CPU times: user 21 ms, sys: 3.39 ms, total: 24.4 ms\n",
      "Wall time: 24.4 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "%time vect.fit([' '.join(d) for d in processed_docs])\n",
    "%time tsne_data = vect.transform([' '.join(d) for d in processed_docs]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 941 ms, total: 17.5 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%time tsne_result = TSNE(learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.664474  ,   8.992035  ],\n",
       "       [ 23.601093  , -15.2017975 ],\n",
       "       [ 11.646137  ,   0.97763896],\n",
       "       [ 23.60182   , -15.202716  ],\n",
       "       [  8.242556  ,   3.2811804 ],\n",
       "       [ 13.225922  ,  -0.97631407],\n",
       "       [  5.6147494 ,   8.802479  ],\n",
       "       [  4.419053  ,   8.055964  ],\n",
       "       [ 12.462818  ,  -1.2678202 ],\n",
       "       [  9.879312  ,   5.7678313 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 시각화\n",
    "# plt.scatter(tsne_result[:, 1], tsne_result[:, 0])\n",
    "# plt.xlim(tsne_result[:, 1].min()-3, tsne_result[:, 1].max()+3) # 최소, 최대\n",
    "# plt.ylim(tsne_result[:, 0].min()-3, tsne_result[:, 0].max()+3) # 최소, 최대\n",
    "# plt.xlabel('t-SNE 특성0') # x축 이름\n",
    "# plt.ylabel('t-SNE 특성1') # y축 이름\n",
    "# plt.show() # 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 952 ms, total: 25.3 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%time tsne_3d_result = TSNE(n_components=3, learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 103.58746  ,   72.253586 ,    5.65633  ],\n",
       "       [  31.781635 ,   89.92669  ,  -82.63583  ],\n",
       "       [  20.661259 ,  124.87336  ,  -45.061954 ],\n",
       "       [  24.231483 ,   92.08438  , -102.17858  ],\n",
       "       [-146.37094  ,    8.817643 ,  -81.40019  ],\n",
       "       [ 107.89072  ,  130.8689   ,  -55.291992 ],\n",
       "       [ 124.24056  ,   16.723385 ,   -6.6259217],\n",
       "       [ 127.55127  ,   27.194212 ,  -41.71926  ],\n",
       "       [  96.68417  ,  108.18609  ,  -65.152824 ],\n",
       "       [  45.471752 , -115.20811  ,   79.15913  ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_3d_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "# plt.rcParams['lines.linewidth'] = 1\n",
    "# plt.rcParams['lines.color'] = 'r'\n",
    "# plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for x, y, z in tsne_3d_result:\n",
    "#     ax.scatter(x, y, z, c='blue')\n",
    "    \n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgzBTvekcz43"
   },
   "source": [
    "### LDA\n",
    "\n",
    "* Setting Variables\n",
    "\n",
    "    1. document_topic_counts : List of Counter (len = count of documents)\n",
    "    2. topic_word_counts : List of Counter (len = count of topic)\n",
    "    3. topic_counts : List of Integer (len = count of topic)\n",
    "    4. document_lengths : List of length of documents\n",
    "    5. distinct_words: All unique words in dataset\n",
    "    6. V: length of distinct words\n",
    "    7. D: length of documents\n",
    "    \n",
    "* Counter Object\n",
    " - Calculate count of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9LRe-K8bcKK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_variables(K):\n",
    "    # 사용자가 원하는 토픽의 갯수\n",
    "    K = 8\n",
    "\n",
    "    # 각 토픽이 각 문서에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 문서를 의미\n",
    "    document_topic_counts = [Counter() for _ in processed_docs]\n",
    "\n",
    "    # 각 단어가 각 토픽에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 토픽을 의미\n",
    "    topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "    # 각 토픽에 할당되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 토픽을 의미함\n",
    "    topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "    # 각 문서에 포함되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 문서를 의미함\n",
    "    document_lengths = list(map(len, processed_docs))\n",
    "\n",
    "    # 단어 종류의 수\n",
    "    distinct_words = set(word for document in processed_docs for word in document)\n",
    "    V = len(distinct_words)\n",
    "\n",
    "    # 총 문서의 수\n",
    "    D = len(processed_docs)\n",
    "\n",
    "    return V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7qgzPvO7kTP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gjsUyxxc4fh"
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3jga4rC7krT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI23O0lUdVO2"
   },
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5lgknpvh5ba"
   },
   "source": [
    "* Run\n",
    " - Initialize Topic using random value by word in documents\n",
    " - Calculate variables\n",
    "    1. document_topic_counts\n",
    "        - count of topic word in every document\n",
    "        - 개별 문서에서 topic word의 등장 횟수\n",
    "    2. topic_word_counts\n",
    "        - appearance count of words in whole documents\n",
    "        - every word seperate by topic\n",
    "        - 개별 Topic에서 topic word의 등장 횟수(전체 문서 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_ViLSFS5bwX"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "K = 8\n",
    "V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words = get_variables(K)\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document] for document in processed_docs]\n",
    "\n",
    "# 위와 같이 랜덤 초기화한 상태에서 \n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(processed_docs[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1541572007570,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "z_qa8yOb2fyC",
    "outputId": "f224e65b-0390-4487-fdcb-22488ff00697"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1400979,
     "status": "ok",
     "timestamp": 1541577464691,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "jxqPxYZv7Usi",
    "outputId": "a82e7172-17a9-4db3-9ca3-568fa8de2b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 iter: 0.005571548144022624 mins ---\n",
      "--- 1 iter: 0.011192631721496583 mins ---\n",
      "--- 2 iter: 0.016779232025146484 mins ---\n",
      "--- 0.01678473154703776 mins ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time() \n",
    "\n",
    "for iter in range(3):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(processed_docs[d], document_topics[d])):\n",
    "            # 깁스 샘플링 수행을 위해\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한 \n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # 샘플링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "    \n",
    "    print(\"--- %d iter: %s mins ---\" % (iter, str((time.time() - start_time) / 60.)))\n",
    "\n",
    "print(\"--- %s mins ---\" % str((time.time() - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 6, 1: 0, 2: 6, 3: 0, 4: 0, 5: 6, 6: 1, 7: 0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## i번째 document의 topic 비중\n",
    "document_topic_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: risk(28)\n",
      "Topic 1: patient(25)\n",
      "Topic 2: \n",
      "Topic 3: treatment(25)\n",
      "Topic 4: year(30),death(30)\n",
      "Topic 5: coronari(30)\n",
      "Topic 6: event(30),advers(30)\n",
      "Topic 7: \n"
     ]
    }
   ],
   "source": [
    "## i번째 topic의 단어 비중\n",
    "for i in range(8):\n",
    "    print('Topic %d: %s' % (i, ','.join(['%s(%s)' % (k, topic_word_counts[i].get(k)) for k in topic_word_counts[i].keys() if topic_word_counts[i].get(k) >= 25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: risk(28),score(23),relat(23),heart(21),combin(18),predict(17),cardiac(15),failur(14),analysi(13),stroke(12)\n",
      "Topic 1: patient(25),advers(18),treatment(14),year(14),clinic(14),event(13),women(13),modern(13),grade(12),model(10)\n",
      "Topic 2: model(24),patient(23),sensit(18),differ(17),hazard(16),standardis(16),surviv(14),year(14),relat(14),cancer(14)\n",
      "Topic 3: treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)\n",
      "Topic 4: year(30),death(30),risk(18),myocardi(18),acut(18),efficaci(17),caus(16),accord(16),infarct(15),patient(14)\n",
      "Topic 5: coronari(30),intervent(17),diseas(12),cancer(10),base(9),peak(9),effect(8),estim(8),control(8),tomographi(8)\n",
      "Topic 6: event(30),advers(30),patient(22),grade(16),mortal(13),regress(11),multivari(11),coronari(10),variabl(10),occur(9)\n",
      "Topic 7: legend(14),analysi(14),patient(12),specif(8),adjust(8),number(8),associ(7),ratio(7),imag(7),rate(7)\n"
     ]
    }
   ],
   "source": [
    "## i번째 topic의 단어 비중\n",
    "for i in range(8):\n",
    "    print('Topic %d: %s' % (i, ','.join(['%s(%s)' % (a, b) for a, b in topic_word_counts[i].most_common(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.color'] = 'r'\n",
    "plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# doc_result.plot.scatter(x='plot_x', y='plot_y', c='topic', colormap='Accent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threedee = plt.figure().gca(projection='3d')\n",
    "# threedee.scatter(doc_result.td_x, doc_result.td_y, doc_result.td_z, c=doc_result.topic)\n",
    "\n",
    "# plt.savefig('3d_scatter_lda.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_prob</th>\n",
       "      <th>topic_word</th>\n",
       "      <th>plot_x</th>\n",
       "      <th>plot_y</th>\n",
       "      <th>td_x</th>\n",
       "      <th>td_y</th>\n",
       "      <th>td_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>23.601820</td>\n",
       "      <td>-15.202716</td>\n",
       "      <td>24.231483</td>\n",
       "      <td>92.084381</td>\n",
       "      <td>-102.178581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>23.601093</td>\n",
       "      <td>-15.201797</td>\n",
       "      <td>31.781635</td>\n",
       "      <td>89.926689</td>\n",
       "      <td>-82.635834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs &gt;14 days)</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>-9.190104</td>\n",
       "      <td>5.256001</td>\n",
       "      <td>11.063636</td>\n",
       "      <td>38.820911</td>\n",
       "      <td>129.074020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>361</td>\n",
       "      <td>Pooled analysis of the early risk of recurrent vascular events, given per time period after randomisation, in trials of aspirin versus control in secondary prevention after transient ischaemic attack and ischaemic stroke</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>-8.648508</td>\n",
       "      <td>4.525882</td>\n",
       "      <td>-0.745511</td>\n",
       "      <td>36.269920</td>\n",
       "      <td>104.871330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>297</td>\n",
       "      <td>Percentage of Total Cross Section Represented by Plaque Area  (Difference Between Lumen Area and Area Delimited by Internal Elastic Lamina)  at Either 7 or 21 Days After Treatment with rhVEGF  (2 g/kg by a Single Intramuscular Injection)  or Albumin legend</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>26.421835</td>\n",
       "      <td>3.316678</td>\n",
       "      <td>-65.335838</td>\n",
       "      <td>-24.417757</td>\n",
       "      <td>-64.738106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "3      3   \n",
       "1      1   \n",
       "373  373   \n",
       "361  361   \n",
       "297  297   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                  document  \\\n",
       "3                                                                             Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "1                                                                                                                                      Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "373  Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs >14 days)   \n",
       "361                                                                                                                                                           Pooled analysis of the early risk of recurrent vascular events, given per time period after randomisation, in trials of aspirin versus control in secondary prevention after transient ischaemic attack and ischaemic stroke   \n",
       "297                                                                                                                       Percentage of Total Cross Section Represented by Plaque Area  (Difference Between Lumen Area and Area Delimited by Internal Elastic Lamina)  at Either 7 or 21 Days After Treatment with rhVEGF  (2 g/kg by a Single Intramuscular Injection)  or Albumin legend   \n",
       "\n",
       "     topic  topic_prob  \\\n",
       "3        3          25   \n",
       "1        3          22   \n",
       "373      3          21   \n",
       "361      3          16   \n",
       "297      3          16   \n",
       "\n",
       "                                                                                                       topic_word  \\\n",
       "3    treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "1    treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "373  treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "361  treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "297  treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "\n",
       "        plot_x     plot_y       td_x       td_y        td_z  \n",
       "3    23.601820 -15.202716  24.231483  92.084381 -102.178581  \n",
       "1    23.601093 -15.201797  31.781635  89.926689  -82.635834  \n",
       "373  -9.190104   5.256001  11.063636  38.820911  129.074020  \n",
       "361  -8.648508   4.525882  -0.745511  36.269920  104.871330  \n",
       "297  26.421835   3.316678 -65.335838 -24.417757  -64.738106  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result[doc_result.topic == 3].sort_values('topic_prob', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_prob</th>\n",
       "      <th>topic_word</th>\n",
       "      <th>plot_x</th>\n",
       "      <th>plot_y</th>\n",
       "      <th>td_x</th>\n",
       "      <th>td_y</th>\n",
       "      <th>td_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>event(30),advers(30),patient(22),grade(16),mortal(13),regress(11),multivari(11),coronari(10),variabl(10),occur(9)</td>\n",
       "      <td>-1.035476</td>\n",
       "      <td>4.404040</td>\n",
       "      <td>-65.114105</td>\n",
       "      <td>-27.710230</td>\n",
       "      <td>-144.433136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>23.601820</td>\n",
       "      <td>-15.202716</td>\n",
       "      <td>24.231483</td>\n",
       "      <td>92.084381</td>\n",
       "      <td>-102.178581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "      <td>23.601093</td>\n",
       "      <td>-15.201797</td>\n",
       "      <td>31.781635</td>\n",
       "      <td>89.926689</td>\n",
       "      <td>-82.635834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Potential effect of the internet on physical activity interventions  (based on effect estimates from web-based physical activity interventions and other physical activity interventions)  and the potential effect of mobile phones on physical activity interventions  (based on effect estimates from telephone-based physical activity interventions and from other physical activity interventions) , by country income</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>coronari(30),intervent(17),diseas(12),cancer(10),base(9),peak(9),effect(8),estim(8),control(8),tomographi(8)</td>\n",
       "      <td>-15.383671</td>\n",
       "      <td>0.556147</td>\n",
       "      <td>45.630489</td>\n",
       "      <td>-75.589569</td>\n",
       "      <td>-66.935539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>201</td>\n",
       "      <td>Decomposition analysis of the change of global disability-adjusted life years  (thousands)  by level 1 causes from 1990 to 2010 into total population growth, population ageing, and changes in age-specific, sex-specific, and cause-specific disability-adjusted-life-year rates</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>year(30),death(30),risk(18),myocardi(18),acut(18),efficaci(17),caus(16),accord(16),infarct(15),patient(14)</td>\n",
       "      <td>-30.504271</td>\n",
       "      <td>-1.075239</td>\n",
       "      <td>115.293465</td>\n",
       "      <td>-109.835449</td>\n",
       "      <td>25.390877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "69    69   \n",
       "3      3   \n",
       "1      1   \n",
       "50    50   \n",
       "201  201   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             document  \\\n",
       "69   Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .   \n",
       "3                                                                                                                                                                                                                                        Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "1                                                                                                                                                                                                                                                                                                 Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "50                                                                                                                       Potential effect of the internet on physical activity interventions  (based on effect estimates from web-based physical activity interventions and other physical activity interventions)  and the potential effect of mobile phones on physical activity interventions  (based on effect estimates from telephone-based physical activity interventions and from other physical activity interventions) , by country income   \n",
       "201                                                                                                                                                                                                                                                                Decomposition analysis of the change of global disability-adjusted life years  (thousands)  by level 1 causes from 1990 to 2010 into total population growth, population ageing, and changes in age-specific, sex-specific, and cause-specific disability-adjusted-life-year rates   \n",
       "\n",
       "     topic  topic_prob  \\\n",
       "69       6          31   \n",
       "3        3          25   \n",
       "1        3          22   \n",
       "50       5          22   \n",
       "201      4          21   \n",
       "\n",
       "                                                                                                            topic_word  \\\n",
       "69   event(30),advers(30),patient(22),grade(16),mortal(13),regress(11),multivari(11),coronari(10),variabl(10),occur(9)   \n",
       "3         treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "1         treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)   \n",
       "50        coronari(30),intervent(17),diseas(12),cancer(10),base(9),peak(9),effect(8),estim(8),control(8),tomographi(8)   \n",
       "201         year(30),death(30),risk(18),myocardi(18),acut(18),efficaci(17),caus(16),accord(16),infarct(15),patient(14)   \n",
       "\n",
       "        plot_x     plot_y        td_x        td_y        td_z  \n",
       "69   -1.035476   4.404040  -65.114105  -27.710230 -144.433136  \n",
       "3    23.601820 -15.202716   24.231483   92.084381 -102.178581  \n",
       "1    23.601093 -15.201797   31.781635   89.926689  -82.635834  \n",
       "50  -15.383671   0.556147   45.630489  -75.589569  -66.935539  \n",
       "201 -30.504271  -1.075239  115.293465 -109.835449   25.390877  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.sort_values('topic_prob', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Glk0FqEpvIBU"
   },
   "source": [
    "* 깁스 샘플링(Gibbs Sampling) \n",
    "    * http://4four.us/article/2014/10/lda-parameter-estimation\n",
    "    * https://bab2min.tistory.com/569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyLDAvis\n",
    "    * https://lovit.github.io/nlp/2018/09/27/pyldavis_lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ndarray, shape = (n_topics, n_terms)\n",
    "topic_term_dists = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))) \n",
    "\n",
    "# numpy.ndarray, shape = (n_docs, n_topics)\n",
    "doc_topic_dists = pd.DataFrame([d.values() for d in document_topic_counts]).fillna(0).values\n",
    "doc_topic_dists = doc_topic_dists + [6.25] * 8\n",
    "doc_topic_dists = sklearn.preprocessing.normalize(doc_topic_dists, norm='l1', axis=1)\n",
    "\n",
    "# numpy.ndarray, shape = (n_docs,)\n",
    "doc_lengths = np.array(document_lengths)\n",
    "\n",
    "# list of str, vocab list\n",
    "vocab = list(distinct_words)\n",
    "\n",
    "# numpy.ndarray, shape = (n_vocabs,)\n",
    "term_frequency = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* topic_term_dists: topic_term_dists\n",
    "* doc_topic_dists: doc_topic_dists\n",
    "* doc_lengths: doc_lengths\n",
    "* vocab: vocab\n",
    "* term_frequency: term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 7],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 4],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 1, 1, 2],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [3, 1, 2, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_term_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 0., 6., ..., 6., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 3.],\n",
       "       [7., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 6., 0., 0.],\n",
       "       [4., 4., 0., ..., 0., 0., 0.],\n",
       "       [8., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([d.values() for d in document_topic_counts]).fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:240: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:241: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "lda_mallet_data = {\n",
    "    'topic_term_dists':topic_term_dists,\n",
    "    'doc_topic_dists':doc_topic_dists,\n",
    "    'doc_lengths':doc_lengths,\n",
    "    'vocab':vocab,\n",
    "    'term_frequency':term_frequency\n",
    "}\n",
    "vis_data = pyLDAvis.prepare(**lda_mallet_data)\n",
    "# pyLDAvis.display(vis_data)\n",
    "# pyLDAvis.save_html(vis_data, 'test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Default' 'Topic1' 'Topic2' 'Topic3' 'Topic4' 'Topic5' 'Topic6' 'Topic7'\n",
      " 'Topic8']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Term</th>\n",
       "      <th>Total</th>\n",
       "      <th>loglift</th>\n",
       "      <th>logprob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>19086.898140</td>\n",
       "      <td>treatment</td>\n",
       "      <td>52818.859371</td>\n",
       "      <td>7.6038</td>\n",
       "      <td>3.2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>17559.946289</td>\n",
       "      <td>event</td>\n",
       "      <td>60135.107296</td>\n",
       "      <td>7.3907</td>\n",
       "      <td>3.1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>16032.994438</td>\n",
       "      <td>popul</td>\n",
       "      <td>31774.648659</td>\n",
       "      <td>7.9377</td>\n",
       "      <td>3.0445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>14506.042586</td>\n",
       "      <td>studi</td>\n",
       "      <td>21253.895208</td>\n",
       "      <td>8.2397</td>\n",
       "      <td>2.9444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>11452.138884</td>\n",
       "      <td>stroke</td>\n",
       "      <td>21026.516902</td>\n",
       "      <td>8.0141</td>\n",
       "      <td>2.7081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category          Freq       Term         Total  loglift  logprob\n",
       "term                                                                  \n",
       "736    Topic1  19086.898140  treatment  52818.859371   7.6038   3.2189\n",
       "752    Topic1  17559.946289      event  60135.107296   7.3907   3.1355\n",
       "423    Topic1  16032.994438      popul  31774.648659   7.9377   3.0445\n",
       "469    Topic1  14506.042586      studi  21253.895208   8.2397   2.9444\n",
       "808    Topic1  11452.138884     stroke  21026.516902   8.0141   2.7081"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDAvis의 우측 HBar Chart Data\n",
    "# Freq: Estimated term frequency within the selected topic\n",
    "# Total: Overall term frequency\n",
    "print(vis_data.topic_info.Category.unique())\n",
    "vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### 1. Main View\n",
    "* Layout: https://www.codingfactory.net/10530\n",
    "\n",
    "#### a. HBar Chart\n",
    "* Data: vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()\n",
    "* D3: http://bl.ocks.org/erikvullings/51cc5332439939f1f292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hbar_json = {}\n",
    "hbar_json['labels'] = vis_data.topic_info.Category.unique().tolist()\n",
    "hbar_json['max_width'] = vis_data.topic_info[vis_data.topic_info.Category != 'Default'][['Total']].max()[0]\n",
    "for l in vis_data.topic_info.Category.unique().tolist():\n",
    "    tmp_df = vis_data.topic_info[vis_data.topic_info.Category == l].sort_values(['Category', 'Freq'], ascending=[True, False]).groupby('Category').head()\n",
    "    sub_json = {}\n",
    "\n",
    "    hbar_json[l] = list(tmp_df[['Term', 'Freq', 'Total']].sort_values('Freq', ascending=False).reset_index().to_dict('index').values())\n",
    "    \n",
    "f = open('./Visualization/res/lda/hbar_data.json', 'w')\n",
    "f.write(json.dumps(hbar_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Scatter Chart\n",
    "* Data: tsne_result\n",
    "* D3: https://bl.ocks.org/Niekes/1c15016ae5b5f11508f92852057136b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17753623, 0.09057971, 0.17753623, ..., 0.17753623, 0.10507246,\n",
       "        0.09057971],\n",
       "       [0.08333333, 0.08333333, 0.08333333, ..., 0.08333333, 0.08333333,\n",
       "        0.12333333],\n",
       "       [0.20384615, 0.09615385, 0.09615385, ..., 0.09615385, 0.09615385,\n",
       "        0.09615385],\n",
       "       ...,\n",
       "       [0.10964912, 0.10964912, 0.10964912, ..., 0.21491228, 0.10964912,\n",
       "        0.10964912],\n",
       "       [0.17372881, 0.17372881, 0.1059322 , ..., 0.1059322 , 0.1059322 ,\n",
       "        0.1059322 ],\n",
       "       [0.23360656, 0.10245902, 0.11885246, ..., 0.10245902, 0.10245902,\n",
       "        0.10245902]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_json = list(doc_result[['id', 'plot_x', 'plot_y', 'topic']].to_dict('index').values())\n",
    "\n",
    "f = open('./Visualization/res/lda/scatter_data.json', 'w')\n",
    "f.write(json.dumps(scatter_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Table\n",
    "* Data: doc_result[['topic', 'document']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result.to_csv('./data_output/lda.tsv', sep='\\t', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>risk(28),score(23),relat(23),heart(21),combin(18),predict(17),cardiac(15),failur(14),analysi(13),stroke(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>patient(25),advers(18),treatment(14),year(14),clinic(14),event(13),women(13),modern(13),grade(12),model(10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>model(24),patient(23),sensit(18),differ(17),hazard(16),standardis(16),surviv(14),year(14),relat(14),cancer(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>event(30),advers(30),patient(22),grade(16),mortal(13),regress(11),multivari(11),coronari(10),variabl(10),occur(9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>legend(14),analysi(14),patient(12),specif(8),adjust(8),number(8),associ(7),ratio(7),imag(7),rate(7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5</td>\n",
       "      <td>coronari(30),intervent(17),diseas(12),cancer(10),base(9),peak(9),effect(8),estim(8),control(8),tomographi(8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4</td>\n",
       "      <td>year(30),death(30),risk(18),myocardi(18),acut(18),efficaci(17),caus(16),accord(16),infarct(15),patient(14)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic  \\\n",
       "0       0   \n",
       "1       3   \n",
       "2       1   \n",
       "5       2   \n",
       "18      6   \n",
       "36      7   \n",
       "42      5   \n",
       "47      4   \n",
       "\n",
       "                                                                                                           topic_word  \n",
       "0         risk(28),score(23),relat(23),heart(21),combin(18),predict(17),cardiac(15),failur(14),analysi(13),stroke(12)  \n",
       "1        treatment(25),event(23),popul(21),studi(19),stroke(15),advers(14),patient(13),area(11),score(10),ischaem(10)  \n",
       "2         patient(25),advers(18),treatment(14),year(14),clinic(14),event(13),women(13),modern(13),grade(12),model(10)  \n",
       "5      model(24),patient(23),sensit(18),differ(17),hazard(16),standardis(16),surviv(14),year(14),relat(14),cancer(14)  \n",
       "18  event(30),advers(30),patient(22),grade(16),mortal(13),regress(11),multivari(11),coronari(10),variabl(10),occur(9)  \n",
       "36                legend(14),analysi(14),patient(12),specif(8),adjust(8),number(8),associ(7),ratio(7),imag(7),rate(7)  \n",
       "42       coronari(30),intervent(17),diseas(12),cancer(10),base(9),peak(9),effect(8),estim(8),control(8),tomographi(8)  \n",
       "47         year(30),death(30),risk(18),myocardi(18),acut(18),efficaci(17),caus(16),accord(16),infarct(15),patient(14)  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').head(1)[['topic', 'topic_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 6, 9, 12, 20, 24, 26, 27, 33, 37, 48, 55, 64, 66, 75, 83, 87, 103, 119, 149, 174, 190, 209, 210, 217, 220, 232, 261, 262, 266, 268, 270, 274, 276, 278, 283, 288, 290, 298, 315, 328, 334, 345, 346, 350, 351, 352, 353, 354, 359, 364, 366, 376, 382, 387, 389, 394, 395, 398, 399]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 4, 11, 14, 16, 17, 22, 30, 31, 46, 52, 62, 81, 93, 94, 98, 114, 120, 121, 123, 124, 125, 129, 131, 133, 143, 144, 147, 150, 157, 164, 171, 172, 173, 179, 181, 183, 184, 194, 215, 226, 233, 239, 244, 245, 252, 259, 277, 285, 331, 333, 343, 349, 384, 386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5, 7, 8, 13, 15, 23, 28, 32, 34, 35, 38, 39, 43, 45, 57, 58, 59, 67, 73, 74, 84, 85, 100, 108, 110, 113, 130, 141, 142, 145, 163, 166, 177, 186, 192, 200, 203, 204, 205, 206, 212, 213, 214, 218, 219, 223, 225, 229, 231, 234, 242, 248, 254, 255, 264, 272, 286, 326, 362, 379, 388]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 3, 10, 65, 79, 90, 101, 104, 105, 107, 115, 116, 122, 126, 128, 136, 137, 139, 140, 165, 182, 198, 221, 241, 251, 256, 280, 282, 289, 296, 297, 300, 332, 338, 341, 358, 361, 363, 365, 367, 368, 370, 373, 374, 377, 378, 380, 390, 393, 396]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[47, 54, 68, 70, 96, 99, 158, 168, 175, 176, 178, 180, 189, 191, 197, 201, 202, 207, 211, 216, 222, 227, 230, 235, 238, 240, 246, 247, 249, 257, 260, 265, 267, 269, 275, 279, 287, 291, 292, 293, 294, 304, 355, 357, 369, 375, 385, 392]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[42, 50, 56, 60, 71, 72, 76, 80, 86, 91, 109, 111, 151, 153, 167, 187, 195, 208, 224, 236, 237, 253, 258, 273, 305, 306, 307, 308, 310, 314, 317, 319, 320, 322, 324, 325, 327, 329, 330, 335, 336, 339, 342, 360, 381, 391]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[18, 19, 21, 25, 29, 41, 44, 49, 61, 69, 78, 82, 92, 95, 97, 102, 106, 112, 117, 118, 127, 132, 134, 135, 138, 146, 148, 160, 169, 185, 188, 263, 271, 284, 299, 301, 302, 311, 318, 323, 344, 347, 371]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[36, 40, 51, 53, 63, 77, 88, 89, 152, 154, 155, 156, 159, 161, 162, 170, 193, 196, 199, 228, 243, 250, 281, 295, 303, 309, 312, 313, 316, 321, 337, 340, 348, 356, 372, 383, 397]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             id\n",
       "topic                                                                                                                                                                                                                                                                                          \n",
       "0      [0, 6, 9, 12, 20, 24, 26, 27, 33, 37, 48, 55, 64, 66, 75, 83, 87, 103, 119, 149, 174, 190, 209, 210, 217, 220, 232, 261, 262, 266, 268, 270, 274, 276, 278, 283, 288, 290, 298, 315, 328, 334, 345, 346, 350, 351, 352, 353, 354, 359, 364, 366, 376, 382, 387, 389, 394, 395, 398, 399]\n",
       "1                             [2, 4, 11, 14, 16, 17, 22, 30, 31, 46, 52, 62, 81, 93, 94, 98, 114, 120, 121, 123, 124, 125, 129, 131, 133, 143, 144, 147, 150, 157, 164, 171, 172, 173, 179, 181, 183, 184, 194, 215, 226, 233, 239, 244, 245, 252, 259, 277, 285, 331, 333, 343, 349, 384, 386]\n",
       "2      [5, 7, 8, 13, 15, 23, 28, 32, 34, 35, 38, 39, 43, 45, 57, 58, 59, 67, 73, 74, 84, 85, 100, 108, 110, 113, 130, 141, 142, 145, 163, 166, 177, 186, 192, 200, 203, 204, 205, 206, 212, 213, 214, 218, 219, 223, 225, 229, 231, 234, 242, 248, 254, 255, 264, 272, 286, 326, 362, 379, 388]\n",
       "3                                            [1, 3, 10, 65, 79, 90, 101, 104, 105, 107, 115, 116, 122, 126, 128, 136, 137, 139, 140, 165, 182, 198, 221, 241, 251, 256, 280, 282, 289, 296, 297, 300, 332, 338, 341, 358, 361, 363, 365, 367, 368, 370, 373, 374, 377, 378, 380, 390, 393, 396]\n",
       "4                                                    [47, 54, 68, 70, 96, 99, 158, 168, 175, 176, 178, 180, 189, 191, 197, 201, 202, 207, 211, 216, 222, 227, 230, 235, 238, 240, 246, 247, 249, 257, 260, 265, 267, 269, 275, 279, 287, 291, 292, 293, 294, 304, 355, 357, 369, 375, 385, 392]\n",
       "5                                                                  [42, 50, 56, 60, 71, 72, 76, 80, 86, 91, 109, 111, 151, 153, 167, 187, 195, 208, 224, 236, 237, 253, 258, 273, 305, 306, 307, 308, 310, 314, 317, 319, 320, 322, 324, 325, 327, 329, 330, 335, 336, 339, 342, 360, 381, 391]\n",
       "6                                                                                      [18, 19, 21, 25, 29, 41, 44, 49, 61, 69, 78, 82, 92, 95, 97, 102, 106, 112, 117, 118, 127, 132, 134, 135, 138, 146, 148, 160, 169, 185, 188, 263, 271, 284, 299, 301, 302, 311, 318, 323, 344, 347, 371]\n",
       "7                                                                                                             [36, 40, 51, 53, 63, 77, 88, 89, 152, 154, 155, 156, 159, 161, 162, 170, 193, 196, 199, 228, 243, 250, 281, 295, 303, 309, 312, 313, 316, 321, 337, 340, 348, 356, 372, 383, 397]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').agg({'id': 'unique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Topic_Modelling_LDA_ABC_News.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
