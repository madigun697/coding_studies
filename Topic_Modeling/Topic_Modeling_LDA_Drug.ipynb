{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TsKdaZaYb6S"
   },
   "source": [
    "# Topic Modeling using LDA\n",
    "\n",
    "### References\n",
    "\n",
    "* Data: Drug Dataset (400EA)\n",
    "* Preprocess: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "* LDA: https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/09/lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBHXrVXj9rsK"
   },
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ut-GgvMmTiG6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "news_data = pd.read_csv('./mallet_top_sen.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1541566038639,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "B6iJDggyXxMH",
    "outputId": "5391d2f4-6a96-4566-c58e-925f7375d47a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contribu</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>44029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2935</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>['hazard', 'ratio', 'confid', 'interv', 'univari', 'multivari', 'analysi', 'predictor', 'major', 'cardiac', 'event', 'cardiac', 'death', 'worsen', 'heart', 'failur', 'lead', 'heart', 'transplant']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2836</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>['leav', 'ventricular', 'right', 'ventricular', 'eject', 'fraction', 'leav', 'ventricular', 'right', 'ventricular', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'ventricl', 'case', 'leav', 'side', 'wpw']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>41163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2817</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>['partial', 'regress', 'coeffici', 'subject', 'forward', 'stepwis', 'linear', 'regress', 'depend', 'variabl', 'augment', 'pressur', 'augment', 'index']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2797</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>['leav', 'ventricular', 'lvef', 'right', 'ventricular', 'rvef', 'eject', 'fraction', 'leav', 'ventricular', 'lvmp', 'right', 'ventricular', 'rvmp', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'lvpsd', 'rvpsd', 'ventricl', 'case', 'right', 'side', 'wpw']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart</td>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>['predictor', 'mortal', 'multivari', 'analysi', 'variabl', 'show', 'order', 'enter', 'stepwis', 'cox', 'regress', 'model']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  Topic_Num  Topic_Perc_Contribu  \\\n",
       "0           0  44029        0.0               0.2935   \n",
       "1           1  23344        0.0               0.2836   \n",
       "2           2  41163        0.0               0.2817   \n",
       "3           3  23343        0.0               0.2797   \n",
       "4           4  24968        0.0               0.2782   \n",
       "\n",
       "                                                                          Topic_Keywords  \\\n",
       "0  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "1  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "2  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "3  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "4  analysi, multivari, regress, variabl, model, predictor, cardiac, time, univari, heart   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                   Text  \n",
       "0                                                                                                                  ['hazard', 'ratio', 'confid', 'interv', 'univari', 'multivari', 'analysi', 'predictor', 'major', 'cardiac', 'event', 'cardiac', 'death', 'worsen', 'heart', 'failur', 'lead', 'heart', 'transplant']  \n",
       "1                                                     ['leav', 'ventricular', 'right', 'ventricular', 'eject', 'fraction', 'leav', 'ventricular', 'right', 'ventricular', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'ventricl', 'case', 'leav', 'side', 'wpw']  \n",
       "2                                                                                                                                                               ['partial', 'regress', 'coeffici', 'subject', 'forward', 'stepwis', 'linear', 'regress', 'depend', 'variabl', 'augment', 'pressur', 'augment', 'index']  \n",
       "3  ['leav', 'ventricular', 'lvef', 'right', 'ventricular', 'rvef', 'eject', 'fraction', 'leav', 'ventricular', 'lvmp', 'right', 'ventricular', 'rvmp', 'mean', 'phase', 'leav', 'right', 'mean', 'phase', 'differ', 'rmp', 'phase', 'standard', 'deviat', 'lvpsd', 'rvpsd', 'ventricl', 'case', 'right', 'side', 'wpw']  \n",
       "4                                                                                                                                                                                            ['predictor', 'mortal', 'multivari', 'analysi', 'variabl', 'show', 'order', 'enter', 'stepwis', 'cox', 'regress', 'model']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePl8Ln9b_8NV"
   },
   "source": [
    "#### Extract target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1541566040044,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "aFIUgkTfZ2Jl",
    "outputId": "a537fb6a-b4a6-4a96-be3e-c4676af07360"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = news_data[['Origin_Text']]\n",
    "data_text['index'] = news_data[['Unnamed: 0']]\n",
    "documents = data_text\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_GA1U3fAECz"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already satisfied: boto3 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: requests in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.45 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from botocore<1.13.0,>=1.12.45->boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages\n",
      "Requirement already satisfied: six in /Users/gracelee/.pyenv/versions/3.5.4/envs/for_jupyter/lib/python3.5/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1541566041043,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EuX1F67TYXFc",
    "outputId": "c2c74f56-a6fb-466b-bb63-28a117cb62d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gracelee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LO8jzrS7AIrp"
   },
   "source": [
    "* Preprocess\n",
    " 1. simple_preprocess: Split Text by whitespace\n",
    " 2. STOPWORDS: Remove stopwords\n",
    " 3. lemmatize_stemming\n",
    " \n",
    "* lemmatize_stemming\n",
    " - Lemmatizing & Stemming Replace word with original form\n",
    " - Lemmatizing consider whether the word exist in the real world\n",
    " - pos means a position of the word\n",
    " - https://m.blog.naver.com/PostView.nhn?blogId=vangarang&logNo=220963244354&proxyReferer=https%3A%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V0FdvKJZezi"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0xJK8DKBJvSq"
   },
   "source": [
    "* Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1541566043352,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "AkEpG_ey86-3",
    "outputId": "f4f392d1-d5b0-436e-db0b-2fee33d68ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Treatment', 'efficacy', 'at', 'week', '36', 'for', 'the', 'modified', 'intention-to-treat', 'population', 'in', 'the', 'open-label', 'period', 'and', 'at', 'week', '88', 'for', 'the', 'modified', 'intention-to-treat', 'subpopulations', 'in', 'the', 'double-blind', 'period']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['treatment', 'efficaci', 'week', 'modifi', 'intent', 'treat', 'popul', 'open', 'label', 'period', 'week', 'modifi', 'intent', 'treat', 'subpopul', 'doubl', 'blind', 'period']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 100].values[0][0]\n",
    "print('original document: ')\n",
    "\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "go6me5u-TzlG"
   },
   "source": [
    "* Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 164587,
     "status": "ok",
     "timestamp": 1541566208089,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "EGbICJX5aIMg",
    "outputId": "f32a035a-c555-4056-86ea-c971f2e07337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 608 ms, sys: 2.81 ms, total: 611 ms\n",
      "Wall time: 612 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                        [hazard, ratio, confid, interv, univari, multivari, analysi, predictor, major, cardiac, event, cardiac, death, worsen, heart, failur, lead, heart, transplant]\n",
       "1                                           [leav, ventricular, right, ventricular, eject, fraction, leav, ventricular, right, ventricular, mean, phase, leav, right, mean, phase, differ, phase, standard, deviat, ventricl, case, leav, side, legend]\n",
       "2                                                                                                                   [partial, regress, coeffici, subject, forward, stepwis, linear, regress, depend, variabl, augment, pressur, augment, index, legend]\n",
       "3    [leav, ventricular, lvef, right, ventricular, rvef, eject, fraction, leav, ventricular, lvmp, right, ventricular, rvmp, mean, phase, leav, right, mean, phase, differ, phase, standard, deviat, lvpsd, rvpsd, ventricl, case, right, side, legend]\n",
       "4                                                                                                                                                         [predictor, mortal, multivari, analysi, variabl, show, order, enter, stepwis, regress, model]\n",
       "5                                                                [logist, regress, model, post, transplant, predictor, normal, leav, ventricular, eject, fraction, lvef, post, transplant, period, refer, group, group, lvef, post, transplant, period]\n",
       "6                                                                                                        [result, surviv, analysi, proport, hazard, model, combin, point, cardiac, mortal, urgent, heart, transplant, readmiss, congest, heart, failur]\n",
       "7                                                                                                    [impact, normal, versus, reduc, leav, ventricular, eject, fraction, mortal, congest, heart, failur, case, result, proport, hazard, regress, model]\n",
       "8                                                                                               [descript, leav, ventricular, eject, fraction, lvef, differ, time, period, time, transplant, evalu, wait, list, transplant, follow, kidney, transplant]\n",
       "9                                                                                                                                                              [univari, regress, analysi, proport, hazard, analysi, predictor, cardiac, death, legend]\n",
       "Name: Origin_Text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time processed_docs = documents['Origin_Text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://datascienceschool.net/view-notebook/3e7aadbf88ed4f0d87a76f9ddc925d69/\n",
    "* https://lumiamitie.github.io/r/python/tsne-for-r-py/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TSNE모델에는 transform 메소드가 없고 fit_transform만 있음\n",
    "# library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents['Origin_Text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 ms, sys: 1.94 ms, total: 28 ms\n",
      "Wall time: 28.3 ms\n",
      "CPU times: user 21.7 ms, sys: 3.83 ms, total: 25.6 ms\n",
      "Wall time: 25.7 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "%time vect.fit([' '.join(d) for d in processed_docs])\n",
    "%time tsne_data = vect.transform([' '.join(d) for d in processed_docs]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 932 ms, total: 17.5 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%time tsne_result = TSNE(learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.664474  ,   8.992035  ],\n",
       "       [ 23.601093  , -15.2017975 ],\n",
       "       [ 11.646137  ,   0.97763896],\n",
       "       [ 23.60182   , -15.202716  ],\n",
       "       [  8.242556  ,   3.2811804 ],\n",
       "       [ 13.225922  ,  -0.97631407],\n",
       "       [  5.6147494 ,   8.802479  ],\n",
       "       [  4.419053  ,   8.055964  ],\n",
       "       [ 12.462818  ,  -1.2678202 ],\n",
       "       [  9.879312  ,   5.7678313 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 시각화\n",
    "# plt.scatter(tsne_result[:, 1], tsne_result[:, 0])\n",
    "# plt.xlim(tsne_result[:, 1].min()-3, tsne_result[:, 1].max()+3) # 최소, 최대\n",
    "# plt.ylim(tsne_result[:, 0].min()-3, tsne_result[:, 0].max()+3) # 최소, 최대\n",
    "# plt.xlabel('t-SNE 특성0') # x축 이름\n",
    "# plt.ylabel('t-SNE 특성1') # y축 이름\n",
    "# plt.show() # 그래프 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 969 ms, total: 25.4 s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%time tsne_3d_result = TSNE(n_components=3, learning_rate=300, init='pca').fit_transform(np.array(tsne_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 103.58746  ,   72.253586 ,    5.65633  ],\n",
       "       [  31.781635 ,   89.92669  ,  -82.63583  ],\n",
       "       [  20.661259 ,  124.87336  ,  -45.061954 ],\n",
       "       [  24.231483 ,   92.08438  , -102.17858  ],\n",
       "       [-146.37094  ,    8.817643 ,  -81.40019  ],\n",
       "       [ 107.89072  ,  130.8689   ,  -55.291992 ],\n",
       "       [ 124.24056  ,   16.723385 ,   -6.6259217],\n",
       "       [ 127.55127  ,   27.194212 ,  -41.71926  ],\n",
       "       [  96.68417  ,  108.18609  ,  -65.152824 ],\n",
       "       [  45.471752 , -115.20811  ,   79.15913  ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_3d_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "# plt.rcParams['lines.linewidth'] = 1\n",
    "# plt.rcParams['lines.color'] = 'r'\n",
    "# plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# fig = plt.figure(figsize=(8, 6))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for x, y, z in tsne_3d_result:\n",
    "#     ax.scatter(x, y, z, c='blue')\n",
    "    \n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgzBTvekcz43"
   },
   "source": [
    "### LDA\n",
    "\n",
    "* Setting Variables\n",
    "\n",
    "    1. document_topic_counts : List of Counter (len = count of documents)\n",
    "    2. topic_word_counts : List of Counter (len = count of topic)\n",
    "    3. topic_counts : List of Integer (len = count of topic)\n",
    "    4. document_lengths : List of length of documents\n",
    "    5. distinct_words: All unique words in dataset\n",
    "    6. V: length of distinct words\n",
    "    7. D: length of documents\n",
    "    \n",
    "* Counter Object\n",
    " - Calculate count of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9LRe-K8bcKK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_variables(K):\n",
    "    # 사용자가 원하는 토픽의 갯수\n",
    "    K = 8\n",
    "\n",
    "    # 각 토픽이 각 문서에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 문서를 의미\n",
    "    document_topic_counts = [Counter() for _ in processed_docs]\n",
    "\n",
    "    # 각 단어가 각 토픽에 할당되는 횟수\n",
    "    # Counter로 구성된 리스트\n",
    "    # 각 Counter는 각 토픽을 의미\n",
    "    topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "    # 각 토픽에 할당되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 토픽을 의미함\n",
    "    topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "    # 각 문서에 포함되는 총 단어수\n",
    "    # 숫자로 구성된 리스트\n",
    "    # 각각의 숫자는 각 문서를 의미함\n",
    "    document_lengths = list(map(len, processed_docs))\n",
    "\n",
    "    # 단어 종류의 수\n",
    "    distinct_words = set(word for document in processed_docs for word in document)\n",
    "    V = len(distinct_words)\n",
    "\n",
    "    # 총 문서의 수\n",
    "    D = len(processed_docs)\n",
    "\n",
    "    return V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7qgzPvO7kTP"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gjsUyxxc4fh"
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3jga4rC7krT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vI23O0lUdVO2"
   },
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5lgknpvh5ba"
   },
   "source": [
    "* Run\n",
    " - Initialize Topic using random value by word in documents\n",
    " - Calculate variables\n",
    "    1. document_topic_counts\n",
    "        - count of topic word in every document\n",
    "        - 개별 문서에서 topic word의 등장 횟수\n",
    "    2. topic_word_counts\n",
    "        - appearance count of words in whole documents\n",
    "        - every word seperate by topic\n",
    "        - 개별 Topic에서 topic word의 등장 횟수(전체 문서 기준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_ViLSFS5bwX"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "K = 8\n",
    "V, D, document_topic_counts, topic_word_counts, topic_counts, document_lengths, distinct_words = get_variables(K)\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document] for document in processed_docs]\n",
    "\n",
    "# 위와 같이 랜덤 초기화한 상태에서 \n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(processed_docs[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1541572007570,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "z_qa8yOb2fyC",
    "outputId": "f224e65b-0390-4487-fdcb-22488ff00697"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1400979,
     "status": "ok",
     "timestamp": 1541577464691,
     "user": {
      "displayName": "유주형",
      "photoUrl": "https://lh5.googleusercontent.com/-fg1dF67RQts/AAAAAAAAAAI/AAAAAAAAeAQ/W4wNdGVPToE/s64/photo.jpg",
      "userId": "03494144523560780027"
     },
     "user_tz": -540
    },
    "id": "jxqPxYZv7Usi",
    "outputId": "a82e7172-17a9-4db3-9ca3-568fa8de2b99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0 iter: 0.005339932441711426 mins ---\n",
      "--- 1 iter: 0.0106293519337972 mins ---\n",
      "--- 2 iter: 0.016190083821614583 mins ---\n",
      "--- 0.016196115811665853 mins ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time() \n",
    "\n",
    "for iter in range(3):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(processed_docs[d], document_topics[d])):\n",
    "            # 깁스 샘플링 수행을 위해\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한 \n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # 샘플링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "    \n",
    "    print(\"--- %d iter: %s mins ---\" % (iter, str((time.time() - start_time) / 60.)))\n",
    "\n",
    "print(\"--- %s mins ---\" % str((time.time() - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 15, 1: 0, 2: 0, 3: 0, 4: 2, 5: 0, 6: 2, 7: 0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## i번째 document의 topic 비중\n",
    "document_topic_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: regress(33),model(31)\n",
      "Topic 1: \n",
      "Topic 2: cancer(27),year(25)\n",
      "Topic 3: score(37),risk(49),stroke(27)\n",
      "Topic 4: death(34),year(36),caus(33)\n",
      "Topic 5: coronari(47)\n",
      "Topic 6: grade(30),patient(26),advers(70),popul(28),group(29),event(78),treatment(66)\n",
      "Topic 7: patient(57)\n"
     ]
    }
   ],
   "source": [
    "## i번째 topic의 단어 비중\n",
    "for i in range(8):\n",
    "    print('Topic %d: %s' % (i, ','.join(['%s(%s)' % (k, topic_word_counts[i].get(k)) for k in topic_word_counts[i].keys() if topic_word_counts[i].get(k) >= 25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)\n",
      "Topic 1: intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)\n",
      "Topic 2: cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)\n",
      "Topic 3: risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)\n",
      "Topic 4: year(36),death(34),caus(33),rat(19),efficaci(19),specif(17),mortal(16),relat(16),adjust(16),chang(13)\n",
      "Topic 5: coronari(47),arteri(17),patient(16),wall(15),detect(15),angiographi(13),flow(13),diagnost(13),segment(12),comput(12)\n",
      "Topic 6: event(78),advers(70),treatment(66),grade(30),group(29),popul(28),patient(26),safeti(22),emerg(22),relat(17)\n",
      "Topic 7: patient(57),outcom(24),hospit(22),acut(21),clinic(21),legend(20),myocardi(18),infarct(17),therapi(13),versus(11)\n"
     ]
    }
   ],
   "source": [
    "## i번째 topic의 단어 비중\n",
    "for i in range(8):\n",
    "    print('Topic %d: %s' % (i, ','.join(['%s(%s)' % (a, b) for a, b in topic_word_counts[i].most_common(10)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin_Text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                    Origin_Text  \\\n",
       "0                                                                                                   Hazard Ratio  (and 95% Confidence Intervals)  in Univariate and Multivariate Analysis of Predictors of Major Cardiac Events  (Cardiac Death or Worsening of Heart Failure Leading to Heart Transplantation)   \n",
       "1                                                           Left Ventricular and Right Ventricular Ejection Fractions, Left Ventricular and Right Ventricular Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations for Both Ventricles in 30 Cases of Left Sided WPW legend   \n",
       "2                                                                                                                               Partial Regression Coefficients  (All Subjects, n = 262)  for Forward Stepwise Linear Regression for Dependent Variables Augmentation Pressure and Augmentation Index    legend   \n",
       "3  Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "4                                                                                                                                                                              Predictors of Mortality by Multivariable Analysis: Variables Are Shown in the Order They Entered a Stepwise Cox Regression Model   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.color'] = 'r'\n",
    "plt.rcParams['axes.grid'] = True \n",
    "\n",
    "# doc_result.plot.scatter(x='plot_x', y='plot_y', c='topic', colormap='Accent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threedee = plt.figure().gca(projection='3d')\n",
    "# threedee.scatter(doc_result.td_x, doc_result.td_y, doc_result.td_z, c=doc_result.topic)\n",
    "\n",
    "# plt.savefig('3d_scatter_lda.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_prob</th>\n",
       "      <th>topic_word</th>\n",
       "      <th>plot_x</th>\n",
       "      <th>plot_y</th>\n",
       "      <th>td_x</th>\n",
       "      <th>td_y</th>\n",
       "      <th>td_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-1.035476</td>\n",
       "      <td>4.404040</td>\n",
       "      <td>-65.114105</td>\n",
       "      <td>-27.710230</td>\n",
       "      <td>-144.433136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs &gt;14 days)</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-9.190104</td>\n",
       "      <td>5.256001</td>\n",
       "      <td>11.063636</td>\n",
       "      <td>38.820911</td>\n",
       "      <td>129.074020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>374</td>\n",
       "      <td>Prevalence of Solo and Combination Antiplatelet Prescription in AF Patients at Intermediate to High Risk of Stroke, Stratified by Prescription of Aspirin Versus OAC in the Overall CHADS2 Score2 and CHA2DS2-VASc Score2 Cohorts and Subcohorts of Patients With and Without Any Coronary Heart Disease Risk Equivalent</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-14.166582</td>\n",
       "      <td>-7.581415</td>\n",
       "      <td>101.997124</td>\n",
       "      <td>17.345430</td>\n",
       "      <td>128.073715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>361</td>\n",
       "      <td>Pooled analysis of the early risk of recurrent vascular events, given per time period after randomisation, in trials of aspirin versus control in secondary prevention after transient ischaemic attack and ischaemic stroke</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-8.648508</td>\n",
       "      <td>4.525882</td>\n",
       "      <td>-0.745511</td>\n",
       "      <td>36.269920</td>\n",
       "      <td>104.871330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>355</td>\n",
       "      <td>ORunadjusted and ORadj-2 of morbidity at 30 days for patients with anaemia or risk factors and anaemia and risk factors, compared with reference populations of patients with neither anaemia nor risk factors</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-8.367199</td>\n",
       "      <td>-24.744467</td>\n",
       "      <td>29.900482</td>\n",
       "      <td>-182.552826</td>\n",
       "      <td>-22.906118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "69    69   \n",
       "373  373   \n",
       "374  374   \n",
       "361  361   \n",
       "355  355   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             document  \\\n",
       "69   Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .   \n",
       "373                                                                                                                                                             Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs >14 days)   \n",
       "374                                                                                                                                                                                                                          Prevalence of Solo and Combination Antiplatelet Prescription in AF Patients at Intermediate to High Risk of Stroke, Stratified by Prescription of Aspirin Versus OAC in the Overall CHADS2 Score2 and CHA2DS2-VASc Score2 Cohorts and Subcohorts of Patients With and Without Any Coronary Heart Disease Risk Equivalent   \n",
       "361                                                                                                                                                                                                                                                                                                                      Pooled analysis of the early risk of recurrent vascular events, given per time period after randomisation, in trials of aspirin versus control in secondary prevention after transient ischaemic attack and ischaemic stroke   \n",
       "355                                                                                                                                                                                                                                                                                                                                    ORunadjusted and ORadj-2 of morbidity at 30 days for patients with anaemia or risk factors and anaemia and risk factors, compared with reference populations of patients with neither anaemia nor risk factors   \n",
       "\n",
       "     topic  topic_prob  \\\n",
       "69       3          34   \n",
       "373      3          33   \n",
       "374      3          23   \n",
       "361      3          19   \n",
       "355      3          18   \n",
       "\n",
       "                                                                                                    topic_word  \\\n",
       "69   risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "373  risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "374  risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "361  risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "355  risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "\n",
       "        plot_x     plot_y        td_x        td_y        td_z  \n",
       "69   -1.035476   4.404040  -65.114105  -27.710230 -144.433136  \n",
       "373  -9.190104   5.256001   11.063636   38.820911  129.074020  \n",
       "374 -14.166582  -7.581415  101.997124   17.345430  128.073715  \n",
       "361  -8.648508   4.525882   -0.745511   36.269920  104.871330  \n",
       "355  -8.367199 -24.744467   29.900482 -182.552826  -22.906118  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result[doc_result.topic == 3].sort_values('topic_prob', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_prob</th>\n",
       "      <th>topic_word</th>\n",
       "      <th>plot_x</th>\n",
       "      <th>plot_y</th>\n",
       "      <th>td_x</th>\n",
       "      <th>td_y</th>\n",
       "      <th>td_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>Potential effect of the internet on physical activity interventions  (based on effect estimates from web-based physical activity interventions and other physical activity interventions)  and the potential effect of mobile phones on physical activity interventions  (based on effect estimates from telephone-based physical activity interventions and from other physical activity interventions) , by country income</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)</td>\n",
       "      <td>-15.383671</td>\n",
       "      <td>0.556147</td>\n",
       "      <td>45.630489</td>\n",
       "      <td>-75.589569</td>\n",
       "      <td>-66.935539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-1.035476</td>\n",
       "      <td>4.404040</td>\n",
       "      <td>-65.114105</td>\n",
       "      <td>-27.710230</td>\n",
       "      <td>-144.433136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>373</td>\n",
       "      <td>Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs &gt;14 days)</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "      <td>-9.190104</td>\n",
       "      <td>5.256001</td>\n",
       "      <td>11.063636</td>\n",
       "      <td>38.820911</td>\n",
       "      <td>129.074020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)</td>\n",
       "      <td>23.601820</td>\n",
       "      <td>-15.202716</td>\n",
       "      <td>24.231483</td>\n",
       "      <td>92.084381</td>\n",
       "      <td>-102.178581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>231</td>\n",
       "      <td>Age-standardised 5-year net survival  (%)  with 95% CI: adults  (1599 years)  diagnosed with one of seven common malignancies  (breast, cervix, ovary, prostate, brain, myeloid, and lymphoid)  and children  (014 years)  diagnosed with one of three common malignancies  (brain, acute lymphoblastic leukaemia, and lymphoma)  by continent, country, and calendar period of diagnosis</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)</td>\n",
       "      <td>2.444566</td>\n",
       "      <td>23.140268</td>\n",
       "      <td>39.758190</td>\n",
       "      <td>-136.395447</td>\n",
       "      <td>-16.967594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "50    50   \n",
       "69    69   \n",
       "373  373   \n",
       "3      3   \n",
       "231  231   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             document  \\\n",
       "50                                                                                                                       Potential effect of the internet on physical activity interventions  (based on effect estimates from web-based physical activity interventions and other physical activity interventions)  and the potential effect of mobile phones on physical activity interventions  (based on effect estimates from telephone-based physical activity interventions and from other physical activity interventions) , by country income   \n",
       "69   Antibodies to neuronal antigens in cerebellar syndromes   Sera were screened by routine immunohistochemistry on frozen sections of rat cerebellum and positive staining patterns1 were confirmed, as appropriate, by western blotting on rat cerebellar extracts or recombinant Hu or Yo polypeptides. VGCC antibodies were measured by immunoprecipitation of 125I--conotoxin MVIIC-labelled VGCCs extracted from human cerebellum,  2   and antibodies to glutamic acid decarboxylase measured with a commercial kit  (RSR Ltd, Cardiff, UK) .   \n",
       "373                                                                                                                                                             Pooled analysis of the effect of any aspirin versus control in secondary prevention after TIA and ischaemic stroke on the early risk of any recurrent ischaemic stroke and on disabling or fatal ischaemic stroke stratified by the nature of the presenting event  (TIA and minor stroke vs major stroke)  and by time from presenting event to randomisation  (14 days vs >14 days)   \n",
       "3                                                                                                                                                                                                                                        Left Ventricular  (LVEF)  and Right Ventricular  (RVEF)  Ejection Fractions, Left Ventricular  (LVMP)  and Right Ventricular  (RVMP)  Mean Phases, Left-to-Right Mean Phase Difference  (L-RMP)  and Phase Standard Deviations  (LVPSD and RVPSD)  for Both Ventricles in 14 Cases of Right Sided WPW legend   \n",
       "231                                                                                                                                                         Age-standardised 5-year net survival  (%)  with 95% CI: adults  (1599 years)  diagnosed with one of seven common malignancies  (breast, cervix, ovary, prostate, brain, myeloid, and lymphoid)  and children  (014 years)  diagnosed with one of three common malignancies  (brain, acute lymphoblastic leukaemia, and lymphoma)  by continent, country, and calendar period of diagnosis   \n",
       "\n",
       "     topic  topic_prob  \\\n",
       "50       1          36   \n",
       "69       3          34   \n",
       "373      3          33   \n",
       "3        0          31   \n",
       "231      2          30   \n",
       "\n",
       "                                                                                                               topic_word  \\\n",
       "50                 intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)   \n",
       "69              risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "373             risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)   \n",
       "3    regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)   \n",
       "231         cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)   \n",
       "\n",
       "        plot_x     plot_y       td_x        td_y        td_z  \n",
       "50  -15.383671   0.556147  45.630489  -75.589569  -66.935539  \n",
       "69   -1.035476   4.404040 -65.114105  -27.710230 -144.433136  \n",
       "373  -9.190104   5.256001  11.063636   38.820911  129.074020  \n",
       "3    23.601820 -15.202716  24.231483   92.084381 -102.178581  \n",
       "231   2.444566  23.140268  39.758190 -136.395447  -16.967594  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.sort_values('topic_prob', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Glk0FqEpvIBU"
   },
   "source": [
    "* 깁스 샘플링(Gibbs Sampling) \n",
    "    * http://4four.us/article/2014/10/lda-parameter-estimation\n",
    "    * https://bab2min.tistory.com/569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyLDAvis\n",
    "    * https://lovit.github.io/nlp/2018/09/27/pyldavis_lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ndarray, shape = (n_topics, n_terms)\n",
    "topic_term_dists = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))) \n",
    "\n",
    "# numpy.ndarray, shape = (n_docs, n_topics)\n",
    "doc_topic_dists = pd.DataFrame([d.values() for d in document_topic_counts]).fillna(0).values\n",
    "doc_topic_dists = doc_topic_dists + [6.25] * 8\n",
    "doc_topic_dists = sklearn.preprocessing.normalize(doc_topic_dists, norm='l1', axis=1)\n",
    "\n",
    "# numpy.ndarray, shape = (n_docs,)\n",
    "doc_lengths = np.array(document_lengths)\n",
    "\n",
    "# list of str, vocab list\n",
    "vocab = list(distinct_words)\n",
    "\n",
    "# numpy.ndarray, shape = (n_vocabs,)\n",
    "term_frequency = np.array([topic_word_counts[i][k] for i in range(K) for k in list(distinct_words)]).reshape((K, len(distinct_words))).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* topic_term_dists: topic_term_dists\n",
    "* doc_topic_dists: doc_topic_dists\n",
    "* doc_lengths: doc_lengths\n",
    "* vocab: vocab\n",
    "* term_frequency: term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c587539e940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_topic_counts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame([d.values() for d in document_topic_counts]).fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:223: RuntimeWarning: divide by zero encountered in log\n",
      "  kernel = (topic_given_term * np.log((topic_given_term.T / topic_proportion).T))\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:240: RuntimeWarning: divide by zero encountered in log\n",
      "  log_lift = np.log(topic_term_dists / term_proportion)\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:241: RuntimeWarning: divide by zero encountered in log\n",
      "  log_ttd = np.log(topic_term_dists)\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "lda_mallet_data = {\n",
    "    'topic_term_dists':topic_term_dists,\n",
    "    'doc_topic_dists':doc_topic_dists,\n",
    "    'doc_lengths':doc_lengths,\n",
    "    'vocab':vocab,\n",
    "    'term_frequency':term_frequency\n",
    "}\n",
    "vis_data = pyLDAvis.prepare(**lda_mallet_data)\n",
    "# pyLDAvis.display(vis_data)\n",
    "# pyLDAvis.save_html(vis_data, 'test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Default' 'Topic1' 'Topic2' 'Topic3' 'Topic4' 'Topic5' 'Topic6' 'Topic7'\n",
      " 'Topic8']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Freq</th>\n",
       "      <th>Term</th>\n",
       "      <th>Total</th>\n",
       "      <th>loglift</th>\n",
       "      <th>logprob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>503910.0</td>\n",
       "      <td>regress</td>\n",
       "      <td>503910.0</td>\n",
       "      <td>8.4280</td>\n",
       "      <td>3.4965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>473370.0</td>\n",
       "      <td>model</td>\n",
       "      <td>548574.0</td>\n",
       "      <td>8.2806</td>\n",
       "      <td>3.4340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>366480.0</td>\n",
       "      <td>hazard</td>\n",
       "      <td>426575.0</td>\n",
       "      <td>8.2762</td>\n",
       "      <td>3.1781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>351210.0</td>\n",
       "      <td>multivari</td>\n",
       "      <td>361067.0</td>\n",
       "      <td>8.4004</td>\n",
       "      <td>3.1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>Topic1</td>\n",
       "      <td>351210.0</td>\n",
       "      <td>analysi</td>\n",
       "      <td>638765.0</td>\n",
       "      <td>7.8299</td>\n",
       "      <td>3.1355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category      Freq       Term     Total  loglift  logprob\n",
       "term                                                          \n",
       "291    Topic1  503910.0    regress  503910.0   8.4280   3.4965\n",
       "656    Topic1  473370.0      model  548574.0   8.2806   3.4340\n",
       "749    Topic1  366480.0     hazard  426575.0   8.2762   3.1781\n",
       "1185   Topic1  351210.0  multivari  361067.0   8.4004   3.1355\n",
       "768    Topic1  351210.0    analysi  638765.0   7.8299   3.1355"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDAvis의 우측 HBar Chart Data\n",
    "# Freq: Estimated term frequency within the selected topic\n",
    "# Total: Overall term frequency\n",
    "print(vis_data.topic_info.Category.unique())\n",
    "vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### 1. Main View\n",
    "* Layout: https://www.codingfactory.net/10530\n",
    "\n",
    "#### a. HBar Chart\n",
    "* Data: vis_data.topic_info[vis_data.topic_info.Category == 'Topic1'].sort_values('Freq', ascending=False).head()\n",
    "* D3: http://bl.ocks.org/erikvullings/51cc5332439939f1f292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "hbar_json = {}\n",
    "hbar_json['labels'] = vis_data.topic_info.Category.unique().tolist()\n",
    "hbar_json['max_width'] = vis_data.topic_info[vis_data.topic_info.Category != 'Default'][['Total']].max()[0]\n",
    "for l in vis_data.topic_info.Category.unique().tolist():\n",
    "    tmp_df = vis_data.topic_info[vis_data.topic_info.Category == l].sort_values(['Category', 'Freq'], ascending=[True, False]).groupby('Category').head()\n",
    "    sub_json = {}\n",
    "\n",
    "    hbar_json[l] = list(tmp_df[['Term', 'Freq', 'Total']].sort_values('Freq', ascending=False).reset_index().to_dict('index').values())\n",
    "    \n",
    "f = open('./Visualization/res/lda/hbar_data.json', 'w')\n",
    "f.write(json.dumps(hbar_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Scatter Chart\n",
    "* Data: tsne_result\n",
    "* D3: https://bl.ocks.org/Niekes/1c15016ae5b5f11508f92852057136b5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/gracelee/.pyenv/versions/for_jupyter/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "doc_result = documents[['index', 'Origin_Text']]\n",
    "doc_result.columns = ['id', 'document']\n",
    "doc_result['topic'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[0])\n",
    "doc_result['topic_prob'] = doc_result.id.apply(lambda x: max(document_topic_counts[x].items(), key=operator.itemgetter(1))[1])\n",
    "doc_result['topic_word'] = doc_result.topic.apply(lambda x: ','.join(['%s(%s)' % (a, b)for a, b in topic_word_counts[x].most_common(10)]))\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_result, columns=['plot_x', 'plot_y']), left_index=True, right_index=True)\n",
    "doc_result = pd.merge(doc_result, pd.DataFrame(tsne_3d_result, columns=['td_x', 'td_y', 'td_z']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_json = list(doc_result[['id', 'plot_x', 'plot_y', 'topic']].to_dict('index').values())\n",
    "\n",
    "f = open('./Visualization/res/lda/scatter_data.json', 'w')\n",
    "f.write(json.dumps(scatter_json, indent=4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Table\n",
    "* Data: doc_result[['topic', 'document']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_result.to_csv('./data_output/lda.tsv', sep='\\t', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>coronari(47),arteri(17),patient(16),wall(15),detect(15),angiographi(13),flow(13),diagnost(13),segment(12),comput(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>event(78),advers(70),treatment(66),grade(30),group(29),popul(28),patient(26),safeti(22),emerg(22),relat(17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>7</td>\n",
       "      <td>patient(57),outcom(24),hospit(22),acut(21),clinic(21),legend(20),myocardi(18),infarct(17),therapi(13),versus(11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>4</td>\n",
       "      <td>year(36),death(34),caus(33),rat(19),efficaci(19),specif(17),mortal(16),relat(16),adjust(16),chang(13)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic  \\\n",
       "0        0   \n",
       "31       5   \n",
       "50       1   \n",
       "51       2   \n",
       "66       6   \n",
       "69       3   \n",
       "150      7   \n",
       "195      4   \n",
       "\n",
       "                                                                                                               topic_word  \n",
       "0    regress(33),model(31),hazard(24),ventricular(23),analysi(23),multivari(23),death(21),valu(21),proport(21),mortal(20)  \n",
       "31   coronari(47),arteri(17),patient(16),wall(15),detect(15),angiographi(13),flow(13),diagnost(13),segment(12),comput(12)  \n",
       "50                 intervent(19),modern(13),women(12),method(12),estim(12),incom(11),level(11),high(9),countri(9),mean(8)  \n",
       "51          cancer(27),year(25),health(15),incid(14),develop(13),surviv(13),ratio(13),standardis(13),type(12),countri(12)  \n",
       "66            event(78),advers(70),treatment(66),grade(30),group(29),popul(28),patient(26),safeti(22),emerg(22),relat(17)  \n",
       "69              risk(49),score(37),stroke(27),factor(19),associ(19),patient(17),vasc(14),ischaem(14),accord(13),studi(12)  \n",
       "150      patient(57),outcom(24),hospit(22),acut(21),clinic(21),legend(20),myocardi(18),infarct(17),therapi(13),versus(11)  \n",
       "195                 year(36),death(34),caus(33),rat(19),efficaci(19),specif(17),mortal(16),relat(16),adjust(16),chang(13)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').head(1)[['topic', 'topic_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 183, 184, 220, 240, 262, 263, 264, 266, 268, 270, 272, 274, 276, 278, 286, 288, 290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[50, 52, 53, 54, 55, 60, 62, 64, 65, 68, 70, 71, 72, 76, 77, 79, 80, 81, 83, 85, 86, 87, 88, 89, 91, 93, 94, 96, 98, 99, 255, 366, 394]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[51, 56, 57, 58, 59, 61, 63, 67, 73, 74, 84, 200, 203, 204, 205, 206, 208, 212, 213, 215, 218, 221, 223, 225, 226, 231, 233, 236, 244, 248, 284, 292, 386]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[69, 163, 251, 256, 280, 282, 283, 289, 296, 297, 298, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 387, 388, 389, 390, 391, 392, 393, 395, 396, 397, 398, 399]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[195, 201, 202, 207, 209, 210, 211, 214, 216, 217, 219, 222, 224, 227, 228, 229, 230, 232, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 249, 257, 260, 265, 269, 279, 287, 291, 293, 294]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[31, 37, 250, 253, 258, 261, 277, 285, 300, 303, 304, 305, 306, 307, 308, 309, 310, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 385]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[66, 75, 78, 82, 90, 92, 95, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 254, 271, 273, 299, 301, 302, 318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 252, 259, 267, 275, 281, 295, 311, 312, 321, 375]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                  id\n",
       "topic                                                                                                                                                                                                                                                                                                                               \n",
       "0                                                        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 183, 184, 220, 240, 262, 263, 264, 266, 268, 270, 272, 274, 276, 278, 286, 288, 290]\n",
       "1                                                                                                                                                                                            [50, 52, 53, 54, 55, 60, 62, 64, 65, 68, 70, 71, 72, 76, 77, 79, 80, 81, 83, 85, 86, 87, 88, 89, 91, 93, 94, 96, 98, 99, 255, 366, 394]\n",
       "2                                                                                                                                                                         [51, 56, 57, 58, 59, 61, 63, 67, 73, 74, 84, 200, 203, 204, 205, 206, 208, 212, 213, 215, 218, 221, 223, 225, 226, 231, 233, 236, 244, 248, 284, 292, 386]\n",
       "3                                            [69, 163, 251, 256, 280, 282, 283, 289, 296, 297, 298, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 387, 388, 389, 390, 391, 392, 393, 395, 396, 397, 398, 399]\n",
       "4                                                                                                                                [195, 201, 202, 207, 209, 210, 211, 214, 216, 217, 219, 222, 224, 227, 228, 229, 230, 232, 234, 235, 237, 238, 239, 241, 242, 243, 245, 246, 247, 249, 257, 260, 265, 269, 279, 287, 291, 293, 294]\n",
       "5                                                            [31, 37, 250, 253, 258, 261, 277, 285, 300, 303, 304, 305, 306, 307, 308, 309, 310, 313, 314, 315, 316, 317, 319, 320, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 385]\n",
       "6      [66, 75, 78, 82, 90, 92, 95, 97, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 254, 271, 273, 299, 301, 302, 318]\n",
       "7                                           [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 252, 259, 267, 275, 281, 295, 311, 312, 321, 375]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result.groupby('topic').agg({'id': 'unique'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Topic_Modelling_LDA_ABC_News.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
